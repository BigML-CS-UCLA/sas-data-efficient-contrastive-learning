{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure latest version of package is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sas-pip\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (1.24.3)\n",
      "Requirement already satisfied: fast-pytorch-kmeans in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sas==1.0) (0.1.6)\n",
      "Requirement already satisfied: pynvml in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from fast-pytorch-kmeans->sas==1.0) (11.4.1)\n",
      "Requirement already satisfied: sympy in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.7.101)\n",
      "Requirement already satisfied: filelock in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torch->sas==1.0) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->sas==1.0) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->sas==1.0) (67.8.0)\n",
      "Requirement already satisfied: lit in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from triton==2.0.0->torch->sas==1.0) (16.0.5)\n",
      "Requirement already satisfied: cmake in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from triton==2.0.0->torch->sas==1.0) (3.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torchvision->sas==1.0) (9.5.0)\n",
      "Requirement already satisfied: requests in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from torchvision->sas==1.0) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from jinja2->torch->sas==1.0) (2.1.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from requests->torchvision->sas==1.0) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sjoshi/anaconda3/envs/clip/lib/python3.10/site-packages (from sympy->torch->sas==1.0) (1.3.0)\n",
      "Building wheels for collected packages: sas\n",
      "  Building wheel for sas (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sas: filename=sas-1.0-py3-none-any.whl size=6302 sha256=c5c8e30bdefe4554829c2ba27dca1e71e004873f55a209b69473a02f44852661\n",
      "  Stored in directory: /home/sjoshi/.cache/pip/wheels/4e/07/53/a089817b38c15451794418a74eb8812ee557a2982d04e9d60a\n",
      "Successfully built sas\n",
      "Installing collected packages: sas\n",
      "  Attempting uninstall: sas\n",
      "    Found existing installation: sas 1.0\n",
      "    Uninstalling sas-1.0:\n",
      "      Successfully uninstalled sas-1.0\n",
      "Successfully installed sas-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sas-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "cifar100 = torchvision.datasets.CIFAR100(\"/data/cifar100/\", transform=transforms.ToTensor())\n",
    "device = \"cuda:6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition into approximate latent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas.approx_latent_classes import clip_approx\n",
    "from sas.subset_dataset import SASSubsetDataset\n",
    "import random \n",
    "\n",
    "rand_labeled_examples_indices = random.sample(range(len(cifar100)), 500)\n",
    "rand_labeled_examples_labels = [cifar100[i][1] for i in rand_labeled_examples_indices]\n",
    "\n",
    "partition = clip_approx(\n",
    "    img_trainset=cifar100,\n",
    "    labeled_example_indices=rand_labeled_examples_indices, \n",
    "    labeled_examples_labels=rand_labeled_examples_labels,\n",
    "    num_classes=100,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load proxy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "class ProxyModel(nn.Module):\n",
    "    def __init__(self, net, critic):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.critic = critic\n",
    "    def forward(self, x):\n",
    "        return self.critic.project(self.net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Dict, List, Optional\n",
    "import math \n",
    "import pickle\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sas.submodular_maximization import lazy_greedy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Base Subset Dataset (Abstract Base Class)\n",
    "\"\"\"\n",
    "class BaseSubsetDataset(ABC, Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        subset_fraction: float,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset: Original Dataset\n",
    "        :type dataset: Dataset\n",
    "        :param subset_fraction: Fractional size of subset\n",
    "        :type subset_fraction: float\n",
    "        :param verbose: verbose\n",
    "        :type verbose: boolean\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.subset_fraction = subset_fraction\n",
    "        self.len_dataset = len(self.dataset)\n",
    "        self.subset_size = int(self.len_dataset * self.subset_fraction)\n",
    "        self.subset_indices = None\n",
    "        self.verbose = verbose \n",
    "\n",
    "    def initialization_complete(self):\n",
    "        if self.verbose:\n",
    "            print(f\"Subset Size: {self.subset_size}\")\n",
    "            print(f\"Discarded {self.len_dataset - self.subset_size} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.subset_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get the index for the corresponding item in the original dataset\n",
    "        original_index = self.subset_indices[index]\n",
    "        \n",
    "        # Get the item from the original dataset at the corresponding index\n",
    "        original_item = self.dataset[original_index]\n",
    "        \n",
    "        return original_item\n",
    "    \n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self.subset_indices, f)\n",
    "\n",
    "\"\"\"\n",
    "Random Subset\n",
    "\"\"\"\n",
    "class RandomSubsetDataset(BaseSubsetDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        subset_fraction: float,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset: Original Dataset\n",
    "        :type dataset: Dataset\n",
    "        :param subset_fraction: Fractional size of subset\n",
    "        :type subset_fraction: float\n",
    "        :param verbose: verbose\n",
    "        :type verbose: boolean\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dataset=dataset, \n",
    "            subset_fraction=subset_fraction,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.subset_indices = random.sample(range(self.len_dataset), self.subset_size)\n",
    "        self.initialization_complete()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.subset_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get the index for the corresponding item in the original dataset\n",
    "        original_index = self.subset_indices[index]\n",
    "        \n",
    "        # Get the item from the original dataset at the corresponding index\n",
    "        original_item = self.dataset[original_index]\n",
    "        \n",
    "        return original_item\n",
    "\n",
    "\"\"\"\n",
    "Custom Subset\n",
    "\"\"\"\n",
    "class CustomSubsetDataset(BaseSubsetDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        subset_indices: List[int],\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset: Original Dataset\n",
    "        :type dataset: Dataset\n",
    "        :param subset_fraction: Fractional size of subset\n",
    "        :type subset_fraction: float\n",
    "        :param subset_indices: Indices of custom subset\n",
    "        :type subset_indices: List[int]\n",
    "        :param verbose: verbose\n",
    "        :type verbose: boolean\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dataset=dataset, \n",
    "            subset_fraction=1.0,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.subset_size = len(subset_indices)\n",
    "        self.subset_fraction = self.subset_size / len(dataset)\n",
    "        self.subset_indices = subset_indices\n",
    "        self.initialization_complete()\n",
    "\n",
    "\"\"\"\n",
    "Subsets that maximize Augmentation Similarity Subset Dataset\n",
    "\"\"\"\n",
    "class SubsetSelectionObjective:\n",
    "    def __init__(self, distance, threshold=0):\n",
    "        '''\n",
    "        :param distance: (n, n) matrix specifying pairwise augmentation distance\n",
    "        :type distance: np.array\n",
    "        :param threshold: minimum cosine similarity to consider to be significant (default=0)\n",
    "        :type threshold: float\n",
    "        '''\n",
    "        self.distance = distance \n",
    "        self.threshold = threshold\n",
    "\n",
    "    def inc(self, sset, i):\n",
    "        return np.sum(self.distance[i] * (self.distance[i] >= self.threshold)) - np.sum(self.distance[np.ix_(sset, [i])])\n",
    "    \n",
    "    def add(self, i):\n",
    "        self.distance[:][i] = 0\n",
    "        return \n",
    "    \n",
    "class SASSubsetDataset(BaseSubsetDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        subset_fraction: float,\n",
    "        num_downstream_classes: int,\n",
    "        device: torch.device,\n",
    "        approx_latent_class_partition: Dict[int, int],\n",
    "        proxy_model: Optional[nn.Module] = None,\n",
    "        augmentation_distance: Optional[Dict[int, np.array]] = None,\n",
    "        num_runs=1,\n",
    "        pairwise_distance_block_size: int = 1024, \n",
    "        threshold: float = 0.0,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dataset: Dataset\n",
    "            Original dataset for contrastive learning. Assumes that dataset[i] returns a list of augmented views of the original example i.\n",
    "\n",
    "        subset_fraction: float\n",
    "            Fractional size of subset.\n",
    "\n",
    "        num_downstream_classes: int\n",
    "            Number of downstream classes (can be an estimate).\n",
    "\n",
    "        proxy_model: nn.Module\n",
    "            Proxy model to calculate the augmentation distance (and kmeans clustering if the avoid clip option is chosen).\n",
    "\n",
    "        augmentation_distance: Dict[int, np.array]\n",
    "            Pass a precomputed dictionary containing augmentation distance for each latent class.\n",
    "\n",
    "        num_augmentations: int\n",
    "            Number of augmentations to consider while approximating the augmentation distance.\n",
    "\n",
    "        pairwise_distance_block_size: int\n",
    "            Block size for calculating pairwise distance. This is just to optimize GPU usage while calculating pairwise distance and will not affect the subset created in any way.\n",
    "\n",
    "        verbose: boolean\n",
    "            Verbosity of the output.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dataset=dataset, \n",
    "            subset_fraction=subset_fraction,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.device = device\n",
    "        self.num_downstream_classes = num_downstream_classes\n",
    "        self.proxy_model = proxy_model\n",
    "        self.partition = approx_latent_class_partition\n",
    "        self.augmentation_distance = augmentation_distance\n",
    "        self.num_runs = num_runs\n",
    "        self.pairwise_distance_block_size = pairwise_distance_block_size\n",
    "\n",
    "        if self.augmentation_distance == None:\n",
    "            self.augmentation_distance = self.approximate_augmentation_distance()\n",
    "\n",
    "        class_wise_idx = {}\n",
    "        for latent_class in tqdm(self.partition.keys(), desc=\"Subset Selection:\", disable=not verbose):\n",
    "            F = SubsetSelectionObjective(self.augmentation_distance[latent_class].copy(), threshold=threshold)\n",
    "            class_wise_idx[latent_class] = lazy_greedy(F, range(len(self.augmentation_distance[latent_class])), len(self.augmentation_distance[latent_class]))\n",
    "            class_wise_idx[latent_class] = [self.partition[latent_class][i] for i in class_wise_idx[latent_class]]\n",
    "            \n",
    "        self.subset_indices = []\n",
    "        for latent_class in class_wise_idx.keys():\n",
    "            l = len(class_wise_idx[latent_class])\n",
    "            self.subset_indices.extend(class_wise_idx[latent_class][:int(self.subset_fraction * l)])\n",
    "\n",
    "        self.initialization_complete()\n",
    "\n",
    "\n",
    "    def approximate_augmentation_distance(self):\n",
    "        self.proxy_model = self.proxy_model.to(self.device)\n",
    "\n",
    "        # Initialize augmentation distance with all 0s\n",
    "        augmentation_distance = {}\n",
    "        Z = self.encode_trainset()\n",
    "        for latent_class in self.partition.keys():\n",
    "            Z_partition = Z[self.partition[latent_class]]\n",
    "            pairwise_distance = SASSubsetDataset.pairwise_distance(Z_partition, Z_partition)\n",
    "            augmentation_distance[latent_class] = pairwise_distance.copy()\n",
    "        return augmentation_distance\n",
    "\n",
    "    def encode_trainset(self):\n",
    "        trainloader = torch.utils.data.DataLoader(self.dataset, batch_size=self.pairwise_distance_block_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        with torch.no_grad():\n",
    "            Z = []\n",
    "            for input in trainloader:\n",
    "                Z.append(self.proxy_model(input[0].to(self.device)))\n",
    "        return torch.cat(Z, dim=0)\n",
    "    \n",
    "    def encode_augmented_trainset(self, num_positives=1):\n",
    "        trainloader = torch.utils.data.DataLoader(self.dataset, batch_size=self.pairwise_distance_block_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        with torch.no_grad():\n",
    "            Z = []\n",
    "            for _ in range(num_positives):\n",
    "                Z.append([])\n",
    "            for X in trainloader:\n",
    "                for j in range(num_positives):\n",
    "                    Z[j].append(self.proxy_model(X[j].to(self.device)))\n",
    "        for i in range(num_positives):\n",
    "            Z[i] = torch.cat(Z[i], dim=0)\n",
    "        Z = torch.cat(Z, dim=0)\n",
    "        return Z\n",
    "\n",
    "    @staticmethod\n",
    "    def pairwise_distance(Z1: torch.tensor, Z2: torch.tensor, block_size: int = 1024):\n",
    "        similarity_matrices = []\n",
    "        for i in range(Z1.shape[0] // block_size + 1):\n",
    "            similarity_matrices_i = []\n",
    "            e = Z1[i*block_size:(i+1)*block_size]\n",
    "            for j in range(Z2.shape[0] // block_size + 1):\n",
    "                e_t = Z2[j*block_size:(j+1)*block_size].t()\n",
    "                similarity_matrices_i.append(\n",
    "                    np.array(\n",
    "                    torch.cosine_similarity(e[:, :, None], e_t[None, :, :]).detach().cpu()\n",
    "                    )\n",
    "                )\n",
    "            similarity_matrices.append(similarity_matrices_i)\n",
    "        similarity_matrix = np.block(similarity_matrices)\n",
    "\n",
    "        return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset Selection:: 100%|██████████| 100/100 [00:03<00:00, 30.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Size: 10000\n",
      "Discarded 40000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "net = torch.load(\"proxy-cifar100-resnet10-399-net.pt\")\n",
    "critic = torch.load(\"proxy-cifar100-resnet10-399-critic.pt\")\n",
    "proxy_model = ProxyModel(net, critic)\n",
    "     \n",
    "subset_dataset = SASSubsetDataset(\n",
    "    dataset=cifar100,\n",
    "    subset_fraction=0.2,\n",
    "    num_downstream_classes=100,\n",
    "    device=device,\n",
    "    proxy_model=proxy_model,\n",
    "    approx_latent_class_partition=partition,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save subset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset.save_to_file(\"cifar100-0.2-sas-indices.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
